{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3caf4974",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "#try:\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import transformers as ppb  # pytorch transformers\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report as report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "import swifter\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#except Exception as e:\n",
    "#    pass\n",
    "\n",
    "def make_dataframe(input_folder, labels_folder=None):\n",
    "    # MAKE TXT DATAFRAME\n",
    "    text = []\n",
    "\n",
    "    for fil in tqdm(filter(lambda x: x.endswith('.txt'), os.listdir(input_folder))):\n",
    "        iD, txt = fil[7:].split('.')[0], open(input_folder + fil, 'r', encoding='utf-8').read()\n",
    "        text.append((iD, txt))\n",
    "\n",
    "    df_text = pd.DataFrame(text, columns=['id', 'text']).set_index('id')\n",
    "\n",
    "    df = df_text\n",
    "\n",
    "    # MAKE LABEL DATAFRAME\n",
    "    if labels_folder:\n",
    "        labels = pd.read_csv(labels_folder, sep='\\t', header=None)\n",
    "        labels = labels.rename(columns={0: 'id', 1: 'type'})\n",
    "        labels.id = labels.id.apply(str)\n",
    "        labels = labels.set_index('id')\n",
    "\n",
    "        # JOIN\n",
    "        df = labels.join(df_text)[['text', 'type']]\n",
    "\n",
    "    return df\n",
    "\n",
    "class BertTokenizer(object):\n",
    "\n",
    "    def __init__(self, text=[]):\n",
    "        self.text = text\n",
    "\n",
    "        # For DistilBERT:\n",
    "        self.model_class, self.tokenizer_class, self.pretrained_weights = (\n",
    "        ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n",
    "\n",
    "        self.model = self.model_class.from_pretrained(self.pretrained_weights)\n",
    "\n",
    "    def get(self):\n",
    "\n",
    "        df = pd.DataFrame(data={\"text\": self.text})\n",
    "        tokenized = df[\"text\"].swifter.apply((lambda x: self.tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "        max_len = 0\n",
    "        for i in tokenized.values:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = self.model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        features = last_hidden_states[0][:, 0, :].numpy()\n",
    "\n",
    "        return features\n",
    "\n",
    "def main():\n",
    "    print(\"Read Data from disk:\")\n",
    "    #loaddata.load_trainingdata()\n",
    "\n",
    "    language = \"en\"\n",
    "    folder_train = \"../Data/data/\" + language + \"/train-articles-subtask-1/\"\n",
    "    folder_dev = \"../Data/data/\" + language + \"/dev-articles-subtask-1/\"\n",
    "    labels_train_fn = \"../Data/data/\" + language + \"/train-labels-subtask-1.txt\"\n",
    "    out_fn = \"resultsBERT/output-subtask-1-dev-\" + language + \".txt\"\n",
    "\n",
    "    # Read Data\n",
    "    print('Loading training...')\n",
    "    train = make_dataframe(folder_train, labels_train_fn)\n",
    "    print('Loading dev...')\n",
    "    test = make_dataframe(folder_dev)\n",
    "\n",
    "    X_train = train['text'].values\n",
    "    X_test = test['text'].values\n",
    "    Y_train = train['type'].values\n",
    "\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    Y_train = encoder.fit_transform(Y_train)\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
    "\n",
    "    _instance = BertTokenizer(text=X_train)\n",
    "    tokens = _instance.get()\n",
    "\n",
    "    #lr_clf = LogisticRegression()\n",
    "    #lr_clf.fit(tokens, Y_train)\n",
    "\n",
    "    pipe = Pipeline([('vectorizer', CountVectorizer(ngram_range=(10, 10),\n",
    "                                                     analyzer='char')),\n",
    "                      ('RandomForestClassifier', DecisionTreeClassifier(class_weight='balanced', max_depth=None,\n",
    "                                 min_samples_split=2, random_state=0))])\n",
    "\n",
    "    pipe.fit(tokens, Y_train)\n",
    "\n",
    "    print('In-sample Acc: \\t\\t', pipe.score(X_train, Y_train))\n",
    "\n",
    "    Y_pred = pipe.predict(X_test)\n",
    "\n",
    "    out = pd.DataFrame(Y_pred, test.index)\n",
    "    out.to_csv(out_fn, sep='\\t', header=None)\n",
    "    print('Results on: ', out_fn)\n",
    "\n",
    "    #_instance = BertTokenizer(text=x_test)\n",
    "    #tokensTest = _instance.get()\n",
    "\n",
    "    #predicted = lr_clf.predict(tokensTest)\n",
    "\n",
    "    #np.mean(predicted == y_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73777d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
